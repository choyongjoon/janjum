name: Cupscore / Daily Data Sync

# Run daily at 9 AM KST (0 AM UTC) and allow manual trigger
on:
  schedule:
    - cron: '0 0 * * *' # Daily at midnight UTC (9 AM KST)
  workflow_dispatch:
    inputs:
      target_cafe:
        description: 'Target specific cafe (optional)'
        required: false
        type: choice
        options:
          - 'all'
          - 'starbucks'
          - 'compose'
          - 'mega'
          - 'paik'
          - 'ediya'
          - 'twosome'
          - 'coffeebean'
          - 'hollys'
          - 'paulbassett'
          - 'mammoth'
        default: 'all'

jobs:
  # Job 0: Setup dependencies and browsers (shared across all jobs)
  setup:
    runs-on: ubuntu-latest
    name: Setup Dependencies & Browsers
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 10

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '22'
          cache: 'pnpm'
          cache-dependency-path: 'pnpm-lock.yaml'

      - name: Install dependencies
        run: pnpm install
        working-directory: ./subdomain/cupscore

      - name: Cache node_modules
        uses: actions/cache@v4
        with:
          path: node_modules
          key: cupscore-node-modules-${{ runner.os }}-${{ hashFiles('pnpm-lock.yaml') }}

      - name: Install Playwright browsers
        run: pnpm exec playwright install --with-deps chromium
        working-directory: ./subdomain/cupscore

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: cupscore-playwright-${{ runner.os }}-${{ hashFiles('pnpm-lock.yaml') }}

  # Job 1: Crawl data for each cafe
  crawl:
    runs-on: ubuntu-latest
    needs: [setup]
    strategy:
      matrix:
        cafe: ${{ (github.event.inputs.target_cafe == 'all' || github.event.inputs.target_cafe == '' || !github.event.inputs.target_cafe) && fromJSON('["starbucks", "compose", "mega", "paik", "ediya", "twosome", "coffeebean", "hollys", "paulbassett", "mammoth"]') || fromJSON(format('["{0}"]', github.event.inputs.target_cafe)) }}
      fail-fast: false # Don't cancel other jobs if one fails
      max-parallel: 4 # Limit concurrent crawlers to avoid rate limiting

    name: Crawl ${{ matrix.cafe }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 10

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '22'
          cache: 'pnpm'
          cache-dependency-path: 'pnpm-lock.yaml'

      - name: Restore node_modules cache
        uses: actions/cache/restore@v4
        id: node_modules_cache
        with:
          path: node_modules
          key: cupscore-node-modules-${{ runner.os }}-${{ hashFiles('pnpm-lock.yaml') }}

      - name: Restore Playwright browsers cache
        uses: actions/cache/restore@v4
        id: playwright_cache
        with:
          path: ~/.cache/ms-playwright
          key: cupscore-playwright-${{ runner.os }}-${{ hashFiles('pnpm-lock.yaml') }}

      - name: Install dependencies
        run: pnpm install
        working-directory: ./subdomain/cupscore

      - name: Install Playwright browsers
        if: steps.playwright_cache.outputs.cache-hit != 'true'
        run: pnpm exec playwright install --with-deps chromium
        working-directory: ./subdomain/cupscore

      - name: Create crawler outputs directory
        run: mkdir -p actors/crawler/crawler-outputs
        working-directory: ./subdomain/cupscore

      - name: Run crawler for ${{ matrix.cafe }}
        run: pnpm run crawl ${{ matrix.cafe }}
        working-directory: ./subdomain/cupscore
        env:
          NODE_ENV: production

      - name: Check crawler output exists
        run: |
          if ls actors/crawler/crawler-outputs/${{ matrix.cafe }}-*.json 1> /dev/null 2>&1; then
            echo "✅ Crawler output found for ${{ matrix.cafe }}"
            ls -la actors/crawler/crawler-outputs/${{ matrix.cafe }}-*.json
          else
            echo "❌ No crawler output found for ${{ matrix.cafe }}"
            echo "Expected files matching: actors/crawler/crawler-outputs/${{ matrix.cafe }}-*.json"
            ls -la actors/crawler/crawler-outputs/ || echo "crawler-outputs directory doesn't exist"
            exit 1
          fi
        working-directory: ./subdomain/cupscore

      - name: Upload crawler output
        uses: actions/upload-artifact@v4
        with:
          name: crawler-output-${{ matrix.cafe }}
          path: subdomain/cupscore/actors/crawler/crawler-outputs/${{ matrix.cafe }}-*.json
          retention-days: 7

      - name: Log crawl completion
        run: echo "✅ Crawl completed for ${{ matrix.cafe }}"

  # Job 2: Categorize products (runs after crawl completes)
  categorize:
    runs-on: ubuntu-latest
    needs: [crawl]
    if: ${{ always() }}
    strategy:
      matrix:
        cafe: ${{ (github.event.inputs.target_cafe == 'all' || github.event.inputs.target_cafe == '' || !github.event.inputs.target_cafe) && fromJSON('["starbucks", "compose", "mega", "paik", "ediya", "twosome", "coffeebean", "hollys", "paulbassett", "mammoth"]') || fromJSON(format('["{0}"]', github.event.inputs.target_cafe)) }}
      fail-fast: false # Don't cancel other jobs if one fails
      max-parallel: 4 # Limit concurrent categorization jobs

    name: Categorize ${{ matrix.cafe }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 10

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '22'
          cache: 'pnpm'
          cache-dependency-path: 'pnpm-lock.yaml'

      - name: Restore node_modules cache
        uses: actions/cache/restore@v4
        id: node_modules_cache
        with:
          path: node_modules
          key: cupscore-node-modules-${{ runner.os }}-${{ hashFiles('pnpm-lock.yaml') }}

      - name: Install dependencies
        run: pnpm install
        working-directory: ./subdomain/cupscore

      - name: Create crawler outputs directory
        run: mkdir -p actors/crawler/crawler-outputs
        working-directory: ./subdomain/cupscore

      - name: Download crawler output for ${{ matrix.cafe }}
        uses: actions/download-artifact@v4
        continue-on-error: true
        id: download_artifact
        with:
          name: crawler-output-${{ matrix.cafe }}
          path: subdomain/cupscore/actors/crawler/crawler-outputs

      - name: Check for cafe data
        id: check_data
        run: |
          if [[ "${{ steps.download_artifact.outcome }}" == "success" ]] && ls actors/crawler/crawler-outputs/${{ matrix.cafe }}-*.json 1> /dev/null 2>&1; then
            echo "has_data=true" >> $GITHUB_OUTPUT
            echo "✅ Found data files for ${{ matrix.cafe }}"
          else
            echo "has_data=false" >> $GITHUB_OUTPUT
            echo "⚠️ No data files found for ${{ matrix.cafe }} - crawl may have failed"
          fi
        working-directory: ./subdomain/cupscore

      - name: Run categorization for ${{ matrix.cafe }}
        if: steps.check_data.outputs.has_data == 'true'
        run: pnpm run categorize ${{ matrix.cafe }}
        working-directory: ./subdomain/cupscore
        env:
          NODE_ENV: production

      - name: Upload categorized output for ${{ matrix.cafe }}
        if: steps.check_data.outputs.has_data == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: categorized-output-${{ matrix.cafe }}
          path: subdomain/cupscore/actors/crawler/crawler-outputs/${{ matrix.cafe }}-*.json
          retention-days: 7

      - name: Log categorization completion
        if: steps.check_data.outputs.has_data == 'true'
        run: echo "✅ Categorization completed for ${{ matrix.cafe }}"

      - name: Log skip message
        if: steps.check_data.outputs.has_data == 'false'
        run: echo "⏭️ Skipped categorization for ${{ matrix.cafe }} (no data)"

  # Job 3: Upload to database (runs after categorize completes)
  upload:
    runs-on: ubuntu-latest
    needs: [categorize]
    if: ${{ always() }}
    strategy:
      matrix:
        cafe: ${{ (github.event.inputs.target_cafe == 'all' || github.event.inputs.target_cafe == '' || !github.event.inputs.target_cafe) && fromJSON('["starbucks", "compose", "mega", "paik", "ediya", "twosome", "coffeebean", "hollys", "paulbassett", "mammoth"]') || fromJSON(format('["{0}"]', github.event.inputs.target_cafe)) }}
      fail-fast: false # Don't cancel other jobs if one fails
      max-parallel: 2 # Limit concurrent uploads to avoid database overload

    name: Upload ${{ matrix.cafe }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 10

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '22'
          cache: 'pnpm'
          cache-dependency-path: 'pnpm-lock.yaml'

      - name: Restore node_modules cache
        uses: actions/cache/restore@v4
        id: node_modules_cache
        with:
          path: node_modules
          key: cupscore-node-modules-${{ runner.os }}-${{ hashFiles('pnpm-lock.yaml') }}

      - name: Install dependencies
        run: pnpm install
        working-directory: ./subdomain/cupscore

      - name: Create crawler outputs directory
        run: mkdir -p actors/crawler/crawler-outputs
        working-directory: ./subdomain/cupscore

      - name: Download categorized output for ${{ matrix.cafe }}
        uses: actions/download-artifact@v4
        continue-on-error: true
        id: download_artifact
        with:
          name: categorized-output-${{ matrix.cafe }}
          path: subdomain/cupscore/actors/crawler/crawler-outputs

      - name: Check for cafe data
        id: check_data
        run: |
          if [[ "${{ steps.download_artifact.outcome }}" == "success" ]] && ls actors/crawler/crawler-outputs/${{ matrix.cafe }}-*.json 1> /dev/null 2>&1; then
            echo "has_data=true" >> $GITHUB_OUTPUT
            echo "✅ Found data files for ${{ matrix.cafe }}"
          else
            echo "has_data=false" >> $GITHUB_OUTPUT
            echo "⚠️ No data files found for ${{ matrix.cafe }} - crawl or categorization may have failed"
          fi
        working-directory: ./subdomain/cupscore

      - name: Run upload for ${{ matrix.cafe }}
        if: steps.check_data.outputs.has_data == 'true'
        run: pnpm run upload ${{ matrix.cafe }} --download-images
        working-directory: ./subdomain/cupscore
        env:
          VITE_CONVEX_URL: ${{ secrets.VITE_CONVEX_URL }}
          CONVEX_UPLOAD_SECRET: ${{ secrets.CONVEX_UPLOAD_SECRET }}

      - name: Log upload completion
        if: steps.check_data.outputs.has_data == 'true'
        run: echo "✅ Upload completed for ${{ matrix.cafe }}"

      - name: Log skip message
        if: steps.check_data.outputs.has_data == 'false'
        run: echo "⏭️ Skipped upload for ${{ matrix.cafe }} (no data - crawl or categorization may have failed)"

  # Job 4: Cleanup and notification
  cleanup:
    runs-on: ubuntu-latest
    needs: [crawl, categorize, upload]
    if: ${{ always() }}

    name: Cleanup & Summary

    steps:
      - name: Generate summary
        run: |
          echo "## Daily Data Sync Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow Status:**" >> $GITHUB_STEP_SUMMARY
          echo "- Crawl: ${{ needs.crawl.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Categorize: ${{ needs.categorize.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Upload: ${{ needs.upload.result }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Target:** ${{ github.event.inputs.target_cafe || 'all' }}" >> $GITHUB_STEP_SUMMARY
          echo "**Timestamp:** $(date -u)" >> $GITHUB_STEP_SUMMARY

          if [[ "${{ needs.crawl.result }}" == "failure" ]]; then
            echo "❌ Some crawl jobs failed" >> $GITHUB_STEP_SUMMARY
          fi

          if [[ "${{ needs.categorize.result }}" == "failure" ]]; then
            echo "❌ Categorization failed" >> $GITHUB_STEP_SUMMARY
          fi

          if [[ "${{ needs.upload.result }}" == "failure" ]]; then
            echo "❌ Some upload jobs failed" >> $GITHUB_STEP_SUMMARY
          fi

          if [[ "${{ needs.crawl.result }}" == "success" && "${{ needs.categorize.result }}" == "success" && "${{ needs.upload.result }}" == "success" ]]; then
            echo "✅ All jobs completed successfully!" >> $GITHUB_STEP_SUMMARY
          fi
