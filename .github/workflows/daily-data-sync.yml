name: Daily Data Sync

# Run daily at 9 AM KST (0 AM UTC) and allow manual trigger
on:
  schedule:
    - cron: "0 0 * * *" # Daily at midnight UTC (9 AM KST)
  workflow_dispatch:
    inputs:
      target_cafe:
        description: "Target specific cafe (optional)"
        required: false
        type: choice
        options:
          - "all"
          - "starbucks"
          # - "compose" # blocked in CI, synced via Render cron
          - "mega"
          - "paik"
          - "ediya"
          # - 'twosome' # blocks all cloud IPs, must sync manually
          - "coffeebean"
          - "hollys"
          - "paulbassett"
          - "mammoth"
          - "gongcha"
          - "oozy"
          - "theventi"
        default: "all"

jobs:
  # Job 0: Setup dependencies and browsers (shared across all jobs)
  setup:
    runs-on: ubuntu-latest
    name: Setup Dependencies & Browsers

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 10

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22"
          cache: "pnpm"
          cache-dependency-path: "pnpm-lock.yaml"

      - name: Install dependencies
        run: pnpm install

      - name: Cache node_modules
        uses: actions/cache@v4
        with:
          path: node_modules
          key: node-modules-${{ runner.os }}-${{ hashFiles('pnpm-lock.yaml') }}

      - name: Install Playwright browsers
        run: pnpm exec playwright install --with-deps chromium

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: playwright-${{ runner.os }}-${{ hashFiles('pnpm-lock.yaml') }}

  # Job 1: Crawl data for each cafe
  crawl:
    runs-on: ubuntu-latest
    needs: [setup]
    strategy:
      matrix:
        cafe: ${{ (github.event.inputs.target_cafe == 'all' || github.event.inputs.target_cafe == '' || !github.event.inputs.target_cafe) && fromJSON('["starbucks", "mega", "paik", "ediya", "coffeebean", "hollys", "paulbassett", "mammoth", "gongcha", "oozy", "theventi"]') || fromJSON(format('["{0}"]', github.event.inputs.target_cafe)) }}
      fail-fast: false # Don't cancel other jobs if one fails
      max-parallel: 4 # Limit concurrent crawlers to avoid rate limiting

    name: Crawl ${{ matrix.cafe }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 10

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22"
          cache: "pnpm"
          cache-dependency-path: "pnpm-lock.yaml"

      - name: Restore node_modules cache
        uses: actions/cache/restore@v4
        id: node_modules_cache
        with:
          path: node_modules
          key: node-modules-${{ runner.os }}-${{ hashFiles('pnpm-lock.yaml') }}

      - name: Restore Playwright browsers cache
        uses: actions/cache/restore@v4
        id: playwright_cache
        with:
          path: ~/.cache/ms-playwright
          key: playwright-${{ runner.os }}-${{ hashFiles('pnpm-lock.yaml') }}

      - name: Install dependencies
        run: pnpm install

      - name: Install Playwright browsers
        if: steps.playwright_cache.outputs.cache-hit != 'true'
        run: pnpm exec playwright install --with-deps chromium

      - name: Create crawler outputs directory
        run: mkdir -p actors/crawler/crawler-outputs

      - name: Run crawler for ${{ matrix.cafe }}
        run: |
          pnpm run crawl ${{ matrix.cafe }} || {
            echo "❌ Crawler failed for ${{ matrix.cafe }}"
            echo "Possible causes: site returned 403 (blocked), network error, or no products found"
            ls -la actors/crawler/crawler-outputs/ 2>/dev/null || echo "No crawler-outputs directory"
            exit 1
          }
          echo "✅ Crawler output found for ${{ matrix.cafe }}"
          ls -la actors/crawler/crawler-outputs/${{ matrix.cafe }}-*.json
        env:
          NODE_ENV: production
          CI: true

      - name: Upload crawler output
        uses: actions/upload-artifact@v4
        with:
          name: crawler-output-${{ matrix.cafe }}
          path: actors/crawler/crawler-outputs/${{ matrix.cafe }}-*.json
          retention-days: 7

      - name: Log crawl completion
        run: echo "✅ Crawl completed for ${{ matrix.cafe }}"

  # Job 2: Categorize products (runs after crawl completes)
  categorize:
    runs-on: ubuntu-latest
    needs: [crawl]
    if: ${{ always() }}
    strategy:
      matrix:
        cafe: ${{ (github.event.inputs.target_cafe == 'all' || github.event.inputs.target_cafe == '' || !github.event.inputs.target_cafe) && fromJSON('["starbucks", "mega", "paik", "ediya", "coffeebean", "hollys", "paulbassett", "mammoth", "gongcha", "oozy", "theventi"]') || fromJSON(format('["{0}"]', github.event.inputs.target_cafe)) }}
      fail-fast: false # Don't cancel other jobs if one fails
      max-parallel: 4 # Limit concurrent categorization jobs

    name: Categorize ${{ matrix.cafe }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 10

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22"
          cache: "pnpm"
          cache-dependency-path: "pnpm-lock.yaml"

      - name: Restore node_modules cache
        uses: actions/cache/restore@v4
        id: node_modules_cache
        with:
          path: node_modules
          key: node-modules-${{ runner.os }}-${{ hashFiles('pnpm-lock.yaml') }}

      - name: Install dependencies
        run: pnpm install

      - name: Create crawler outputs directory
        run: mkdir -p actors/crawler/crawler-outputs

      - name: Download crawler output for ${{ matrix.cafe }}
        uses: actions/download-artifact@v4
        continue-on-error: true
        id: download_artifact
        with:
          name: crawler-output-${{ matrix.cafe }}
          path: actors/crawler/crawler-outputs

      - name: Check for cafe data
        id: check_data
        run: |
          if [[ "${{ steps.download_artifact.outcome }}" == "success" ]] && ls actors/crawler/crawler-outputs/${{ matrix.cafe }}-*.json 1> /dev/null 2>&1; then
            echo "has_data=true" >> $GITHUB_OUTPUT
            echo "✅ Found data files for ${{ matrix.cafe }}"
          else
            echo "has_data=false" >> $GITHUB_OUTPUT
            echo "⚠️ No data files found for ${{ matrix.cafe }} - crawl may have failed"
          fi

      - name: Run categorization for ${{ matrix.cafe }}
        if: steps.check_data.outputs.has_data == 'true'
        run: pnpm run categorize ${{ matrix.cafe }}
        env:
          NODE_ENV: production

      - name: Upload categorized output for ${{ matrix.cafe }}
        if: steps.check_data.outputs.has_data == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: categorized-output-${{ matrix.cafe }}
          path: actors/crawler/crawler-outputs/${{ matrix.cafe }}-*.json
          retention-days: 7

      - name: Log categorization completion
        if: steps.check_data.outputs.has_data == 'true'
        run: echo "✅ Categorization completed for ${{ matrix.cafe }}"

      - name: Log skip message
        if: steps.check_data.outputs.has_data == 'false'
        run: echo "⏭️ Skipped categorization for ${{ matrix.cafe }} (no data)"

  # Job 3: Upload to database (runs after categorize completes)
  upload:
    runs-on: ubuntu-latest
    needs: [categorize]
    if: ${{ always() }}
    strategy:
      matrix:
        cafe: ${{ (github.event.inputs.target_cafe == 'all' || github.event.inputs.target_cafe == '' || !github.event.inputs.target_cafe) && fromJSON('["starbucks", "mega", "paik", "ediya", "coffeebean", "hollys", "paulbassett", "mammoth", "gongcha", "oozy", "theventi"]') || fromJSON(format('["{0}"]', github.event.inputs.target_cafe)) }}
      fail-fast: false # Don't cancel other jobs if one fails
      max-parallel: 2 # Limit concurrent uploads to avoid database overload

    name: Upload ${{ matrix.cafe }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 10

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22"
          cache: "pnpm"
          cache-dependency-path: "pnpm-lock.yaml"

      - name: Restore node_modules cache
        uses: actions/cache/restore@v4
        id: node_modules_cache
        with:
          path: node_modules
          key: node-modules-${{ runner.os }}-${{ hashFiles('pnpm-lock.yaml') }}

      - name: Install dependencies
        run: pnpm install

      - name: Create crawler outputs directory
        run: mkdir -p actors/crawler/crawler-outputs

      - name: Download categorized output for ${{ matrix.cafe }}
        uses: actions/download-artifact@v4
        continue-on-error: true
        id: download_artifact
        with:
          name: categorized-output-${{ matrix.cafe }}
          path: actors/crawler/crawler-outputs

      - name: Check for cafe data
        id: check_data
        run: |
          if [[ "${{ steps.download_artifact.outcome }}" == "success" ]] && ls actors/crawler/crawler-outputs/${{ matrix.cafe }}-*.json 1> /dev/null 2>&1; then
            echo "has_data=true" >> $GITHUB_OUTPUT
            echo "✅ Found data files for ${{ matrix.cafe }}"
          else
            echo "has_data=false" >> $GITHUB_OUTPUT
            echo "⚠️ No data files found for ${{ matrix.cafe }} - crawl or categorization may have failed"
          fi

      - name: Run upload for ${{ matrix.cafe }}
        if: steps.check_data.outputs.has_data == 'true'
        run: pnpm run upload ${{ matrix.cafe }}
        env:
          VITE_CONVEX_URL: ${{ secrets.VITE_CONVEX_URL }}
          CONVEX_UPLOAD_SECRET: ${{ secrets.CONVEX_UPLOAD_SECRET }}

      - name: Log upload completion
        if: steps.check_data.outputs.has_data == 'true'
        run: echo "✅ Upload completed for ${{ matrix.cafe }}"

      - name: Log skip message
        if: steps.check_data.outputs.has_data == 'false'
        run: echo "⏭️ Skipped upload for ${{ matrix.cafe }} (no data - crawl or categorization may have failed)"

  # Job 4: Cleanup and notification
  cleanup:
    runs-on: ubuntu-latest
    needs: [crawl, categorize, upload]
    if: ${{ always() }}

    name: Cleanup & Summary

    steps:
      - name: Generate summary
        run: |
          echo "## Daily Data Sync Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow Status:**" >> $GITHUB_STEP_SUMMARY
          echo "- Crawl: ${{ needs.crawl.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Categorize: ${{ needs.categorize.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Upload: ${{ needs.upload.result }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Target:** ${{ github.event.inputs.target_cafe || 'all' }}" >> $GITHUB_STEP_SUMMARY
          echo "**Timestamp:** $(date -u)" >> $GITHUB_STEP_SUMMARY

          if [[ "${{ needs.crawl.result }}" == "failure" ]]; then
            echo "❌ Some crawl jobs failed" >> $GITHUB_STEP_SUMMARY
          fi

          if [[ "${{ needs.categorize.result }}" == "failure" ]]; then
            echo "❌ Categorization failed" >> $GITHUB_STEP_SUMMARY
          fi

          if [[ "${{ needs.upload.result }}" == "failure" ]]; then
            echo "❌ Some upload jobs failed" >> $GITHUB_STEP_SUMMARY
          fi

          if [[ "${{ needs.crawl.result }}" == "success" && "${{ needs.categorize.result }}" == "success" && "${{ needs.upload.result }}" == "success" ]]; then
            echo "✅ All jobs completed successfully!" >> $GITHUB_STEP_SUMMARY
          fi
